{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302d2050",
   "metadata": {},
   "source": [
    "# Build TensorRT engine (PT or ONNX → TRT)\n",
    "\n",
    "This notebook lets you create a TensorRT engine without running the full GUI/capture pipeline. It mirrors the DeepStream flow: convert a YOLO `.pt` to ONNX (optional), then use `trtexec` to build the `.engine` that `nvinfer` will reuse.\n",
    "\n",
    "Notes:\n",
    "- Run this with the **inference** venv kernel (`environments/inference/.venv`).\n",
    "- You need TensorRT's `trtexec` on the Jetson (usually `/usr/src/tensorrt/bin/trtexec`).\n",
    "- For `.pt` conversion this uses the `yolo export` CLI from Ultralytics; install `ultralytics` if not already available (or pre-convert to ONNX yourself).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9e136",
   "metadata": {},
   "source": [
    "# what works\n",
    "\n",
    "- this build engine notebook uses its own uv env (see toml) including onnx, trtexec etc, and ultralytics. you can pass a .pt file (says onnx support but that hasnt been validated yet as i'm unsure what export commands were used to generate the existing onnx files that lack a valid .pt).\n",
    "\n",
    "- it will export to onnx and then build the TRT engine at full or half precision (not yet setup for int-8 calibration)\n",
    "\n",
    "- i built a custom parser that, conviently, infers the number of keypoints based on the yolo pose head output dims. it works with both stock 11 pose models that output 17 kpt coco shape as well as our custom mousehouse model with 6 kpts\n",
    "\n",
    "- the engine and onnx files built in this ipynb are currently saved to /home/jetson/Desktop/squeakview/new_models/ with the suffix .engine. (**Update to artifacts/weights for pt and artifacts/onnx for onnx ***)\n",
    "\n",
    "- the config created by the ipynb is saved to /home/jetson/Desktop/squeakview/DeepStream-Yolo/configs/ (*** CORRECT place ***)\n",
    "\n",
    "- The custom parser is at DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/yolo_pose_parser.cpp and is now built into DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so. this new parser with do confidence filtering, letterbox UNPadding, and print to the terminal the output tensor dimensions that. the parser is specific to our cuda 12.6 as well so if used on a new cuda version we will need to change that flag and rebuilt the parser using the nvs make file (easy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b9ab2",
   "metadata": {},
   "source": [
    "# to do\n",
    "\n",
    "\n",
    "\n",
    "- check efficiency of this full pipeline (make sure no uncessary copies, mem pressure etc)\n",
    "\n",
    "- also make the keypints look better\n",
    "\n",
    "- uncap workspace size (wont work at fp32 if capped)\n",
    "\n",
    "- avoid commented lines in the config make as they can be read \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d207b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"/home/jetson/Desktop/SqueakView/DeepStream-Yolo\")\n",
    "PT_DIR = BASE / \"artifacts\" / \"weights\"\n",
    "ONNX_DIR = BASE / \"artifacts\" / \"onnx\"\n",
    "ENGINE_DIR = BASE / \"engines\"\n",
    "\n",
    "# ---- Configure your model here ----------------------------------------\n",
    "# Task: \"pose\" or \"detect\" (controls parser in generated config)\n",
    "TASK = \"pose\"\n",
    "\n",
    "# Path to a .pt or .onnx file (defaults to weights dir)\n",
    "INPUT_MODEL = PT_DIR / \"yolo26npose_distilled_FT.pt\"\n",
    "\n",
    "# Where to write the engine (defaults to ENGINE_DIR/<stem>_<precision>.engine)\n",
    "ENGINE_OUTPUT = None  # or ENGINE_DIR / \"custom.engine\"\n",
    "\n",
    "# Network input shape (min/opt/max). Keep consistent with DeepStream infer-dims.\n",
    "IMG_SIZE = (1, 3, 640, 640)\n",
    "\n",
    "# Precision: \"fp32\", \"fp16\", or \"int8\" (int8 requires calibration cache not covered here)\n",
    "PRECISION = \"fp16\"\n",
    "\n",
    "# Optional custom plugin used by DeepStream YOLO parser (not required for build, but harmless).\n",
    "CUSTOM_PLUGIN = BASE / \"nvdsinfer_custom_impl_Yolo\" / \"libnvdsinfer_custom_impl_Yolo.so\"\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "for d in (PT_DIR, ONNX_DIR, ENGINE_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PREC_SUFFIX = PRECISION.lower()\n",
    "\n",
    "INPUT_MODEL = INPUT_MODEL.expanduser().resolve()\n",
    "if ENGINE_OUTPUT is None:\n",
    "    ENGINE_OUTPUT = (ENGINE_DIR / f\"{INPUT_MODEL.stem}_{PREC_SUFFIX}\").with_suffix(\".engine\")\n",
    "ENGINE_OUTPUT = ENGINE_OUTPUT.expanduser().resolve()\n",
    "ENGINE_OUTPUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Task       :\", TASK)\n",
    "print(\"Input model:\", INPUT_MODEL)\n",
    "print(\"Engine out :\", ENGINE_OUTPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d877c6",
   "metadata": {},
   "source": [
    "# verify trtexec is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c539abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trtexec() -> Path:\n",
    "    candidates = [\n",
    "        Path(os.environ.get(\"TRTEXEC\") or \"\"),\n",
    "        Path(\"/usr/src/tensorrt/bin/trtexec\"),\n",
    "        Path(shutil.which(\"trtexec\") or \"\"),\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if path and path.is_file() and os.access(path, os.X_OK):\n",
    "            return path\n",
    "    raise FileNotFoundError(\"trtexec not found. Set TRTEXEC env var or install TensorRT tools.\")\n",
    "\n",
    "TRTEXEC = find_trtexec()\n",
    "print(\"Using trtexec:\", TRTEXEC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6895b27",
   "metadata": {},
   "source": [
    "# Export .pt --> Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2666b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If INPUT_MODEL is .pt, convert to ONNX using Ultralytics CLI\n",
    "# (skipped if INPUT_MODEL is already ONNX)\n",
    "def convert_pt_to_onnx(pt_path: Path, imgsz: tuple[int, int]) -> Path:\n",
    "    # Ultralytics writes the ONNX next to the .pt by default; we then move/rename to ONNX_DIR with precision suffix\n",
    "    exported_onnx = pt_path.with_suffix(\".onnx\")\n",
    "    onnx_out = (ONNX_DIR / f\"{pt_path.stem}_{PREC_SUFFIX}\").with_suffix(\".onnx\")\n",
    "    yolo_cmd = shutil.which(\"yolo\")\n",
    "    if not yolo_cmd:\n",
    "        raise RuntimeError(\n",
    "            \"Ultralytics 'yolo' CLI not found. Install with 'pip install ultralytics' in this env, \"\n",
    "            \"or set INPUT_MODEL to an existing ONNX path to skip conversion.\"\n",
    "        )\n",
    "    cmd = [\n",
    "        yolo_cmd,\n",
    "        \"export\",\n",
    "        f\"model={pt_path}\",\n",
    "        \"format=onnx\",\n",
    "        f\"imgsz={imgsz[0] if isinstance(imgsz, (tuple, list)) else imgsz}\",\n",
    "        \"simplify=True\",\n",
    "        \"nms=False\"\n",
    "    ]\n",
    "    print(\"Converting .pt → ONNX:\", \" \".join(cmd))\n",
    "    res = subprocess.run(cmd, text=True)\n",
    "    if res.returncode != 0:\n",
    "        raise RuntimeError(\"Ultralytics conversion failed; ensure 'ultralytics' is installed and yolo CLI is in PATH\")\n",
    "    if not exported_onnx.exists():\n",
    "        raise FileNotFoundError(f\"Expected ONNX not found where Ultralytics writes it: {exported_onnx}\")\n",
    "    onnx_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if onnx_out.resolve() != exported_onnx.resolve():\n",
    "        shutil.move(str(exported_onnx), str(onnx_out))\n",
    "    return onnx_out\n",
    "\n",
    "if INPUT_MODEL.suffix.lower() == \".pt\":\n",
    "    INPUT_MODEL = convert_pt_to_onnx(INPUT_MODEL, imgsz=IMG_SIZE[-2:])\n",
    "    print(\"ONNX written to:\", INPUT_MODEL)\n",
    "else:\n",
    "    assert INPUT_MODEL.exists(), f\"Model not found: {INPUT_MODEL}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d099d",
   "metadata": {},
   "source": [
    "# Verify onnx model tensor name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine ONNX input tensor name (DeepStream auto-detects this internally)\n",
    "INPUT_NAME_OVERRIDE = None  # set to e.g. \"images\" to force a name if detection fails\n",
    "\n",
    "def get_onnx_input_name(path: Path) -> str:\n",
    "    try:\n",
    "        import onnx  # type: ignore\n",
    "    except ImportError as exc:\n",
    "        raise RuntimeError(\"onnx is required to auto-detect input name; set INPUT_NAME_OVERRIDE to skip detection\") from exc\n",
    "    model = onnx.load(str(path))\n",
    "    if model.graph.input:\n",
    "        return model.graph.input[0].name\n",
    "    initializers = {init.name for init in model.graph.initializer}\n",
    "    for node in model.graph.node:\n",
    "        for name in node.input:\n",
    "            if name not in initializers:\n",
    "                return name\n",
    "    raise RuntimeError(\"No inputs found in ONNX (graph.input empty and no candidate from nodes)\")\n",
    "\n",
    "INPUT_NAME = INPUT_NAME_OVERRIDE or get_onnx_input_name(INPUT_MODEL)\n",
    "print(\"ONNX input name:\", INPUT_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52cd40",
   "metadata": {},
   "source": [
    "# Build TRT Engine (this may take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the engine with trtexec (TensorRT 10.x flags)\n",
    "import onnx  # ensure onnx is available for shape inspection\n",
    "\n",
    "# Inspect input shape to decide whether to pass explicit shapes\n",
    "model = onnx.load(str(INPUT_MODEL))\n",
    "inp = model.graph.input[0]\n",
    "type_proto = inp.type.tensor_type\n",
    "shape_proto = type_proto.shape\n",
    "\n",
    "dims = []\n",
    "dynamic = False\n",
    "for dim in shape_proto.dim:\n",
    "    if dim.HasField(\"dim_param\") or dim.dim_param:\n",
    "        dynamic = True\n",
    "        dims.append(dim.dim_param or \"-1\")\n",
    "    else:\n",
    "        val = dim.dim_value\n",
    "        if val in (0, None):\n",
    "            dynamic = True\n",
    "            dims.append(\"-1\")\n",
    "        else:\n",
    "            dims.append(str(val))\n",
    "            if val < 0:\n",
    "                dynamic = True\n",
    "\n",
    "shape_str = \"x\".join(dims) if dynamic else \"x\".join(str(v) for v in IMG_SIZE)\n",
    "\n",
    "cmd = [\n",
    "    str(TRTEXEC),\n",
    "    f\"--onnx={INPUT_MODEL}\",\n",
    "    f\"--saveEngine={ENGINE_OUTPUT}\",\n",
    "]\n",
    "\n",
    "# Only add shape profile flags if the model is dynamic\n",
    "if dynamic:\n",
    "    cmd.extend([\n",
    "        f\"--minShapes={INPUT_NAME}:{shape_str}\",\n",
    "        f\"--optShapes={INPUT_NAME}:{shape_str}\",\n",
    "        f\"--maxShapes={INPUT_NAME}:{shape_str}\",\n",
    "    ])\n",
    "\n",
    "prec = PRECISION.lower()\n",
    "if prec == \"fp16\":\n",
    "    cmd.append(\"--fp16\")\n",
    "elif prec == \"int8\":\n",
    "    cmd.append(\"--int8\")\n",
    "elif prec == \"fp32\":\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(\"PRECISION must be one of fp32, fp16, int8\")\n",
    "\n",
    "if CUSTOM_PLUGIN and CUSTOM_PLUGIN.exists():\n",
    "    cmd.append(f\"--staticPlugins={CUSTOM_PLUGIN}\")  # replaces --plugins\n",
    "\n",
    "print(\"Dynamic input:\" if dynamic else \"Static input:\", dims)\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "result = subprocess.run(cmd)\n",
    "print(\"Return code:\", result.returncode)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"trtexec failed; check above logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Auto-generate config without relying on template placeholders\n",
    "CFG_DIR = Path(\"/home/jetson/Desktop/SqueakView/DeepStream-Yolo/configs\")\n",
    "CFG_NAME = f\"{ENGINE_OUTPUT.stem}.txt\"\n",
    "CFG_PATH = CFG_DIR / CFG_NAME\n",
    "\n",
    "is_pose = str(TASK).lower() == \"pose\"\n",
    "parse_func = \"NvDsInferParseYoloV26Pose\" if is_pose else \"NvDsInferParseYolo\"\n",
    "# class labels for detect; kp labels for pose\n",
    "labels_path = Path(\"/home/jetson/Desktop/SqueakView/DeepStream-Yolo/artifacts/labels/mouse_class.txt\")\n",
    "kp_labels_path = Path(\"/home/jetson/Desktop/SqueakView/DeepStream-Yolo/artifacts/labels/mouse_labels.txt\") if is_pose else None\n",
    "\n",
    "network_mode = 2 if PRECISION.lower() == \"fp16\" else 0\n",
    "infer_h, infer_w = IMG_SIZE[-2], IMG_SIZE[-1]\n",
    "class_count = 1  # adjust per model\n",
    "\n",
    "config_text = f\"\"\"[property]\n",
    "gpu-id=0\n",
    "net-scale-factor=0.0039215697906911373\n",
    "model-color-format=0\n",
    "\n",
    "onnx-file={INPUT_MODEL}\n",
    "model-engine-file={ENGINE_OUTPUT}\n",
    "\n",
    "network-mode={network_mode}                  # 0=FP32, 1=INT8, 2=FP16\n",
    "network-type=0                  # detector\n",
    "infer-dims=3;{infer_w};{infer_h}\n",
    "batch-size=1\n",
    "output-tensor-meta=1\n",
    "\n",
    "num-detected-classes={class_count}\n",
    "labelfile-path={labels_path}\n",
    "\"\"\"\n",
    "\n",
    "if is_pose and kp_labels_path:\n",
    "    config_text += f\"pose-kpt-labels-path={kp_labels_path}\\n\"\n",
    "\n",
    "config_text += f\"\"\"parse-bbox-func-name={parse_func}\n",
    "custom-lib-path=/home/jetson/Desktop/SqueakView/DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so\n",
    "engine-create-func-name=NvDsInferYoloCudaEngineGet\n",
    "\"\"\"\n",
    "\n",
    "if is_pose:\n",
    "    config_text += \"pose-draw-threshold=0.5\\n\"\n",
    "\n",
    "config_text += f\"\"\"\n",
    "cluster-mode=2\n",
    "maintain-aspect-ratio=1\n",
    "symmetric-padding=1\n",
    "workspace-size=2048\n",
    "gie-unique-id=1\n",
    "interval=0\n",
    "process-mode=1\n",
    "\n",
    "[class-attrs-all]\n",
    "nms-iou-threshold=0.15\n",
    "pre-cluster-threshold=0.9\n",
    "topk=20\n",
    "\"\"\"\n",
    "\n",
    "CFG_PATH.write_text(config_text)\n",
    "print(f\"Config written to {CFG_PATH}\")\n",
    "print(\"Parser:\", parse_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471e602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
