{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302d2050",
   "metadata": {},
   "source": [
    "# Build TensorRT engine (PT or ONNX â†’ TRT)\n",
    "\n",
    "This notebook lets you create a TensorRT engine without running the full GUI/capture pipeline. It mirrors the DeepStream flow: convert a YOLO `.pt` to ONNX (optional), then use `trtexec` to build the `.engine` that `nvinfer` will reuse.\n",
    "\n",
    "Notes:\n",
    "- Run this with the **inference** venv kernel (`environments/inference/.venv`).\n",
    "- You need TensorRT's `trtexec` on the Jetson (usually `/usr/src/tensorrt/bin/trtexec`).\n",
    "- For `.pt` conversion this uses the `yolo export` CLI from Ultralytics; install `ultralytics` if not already available (or pre-convert to ONNX yourself).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9e136",
   "metadata": {},
   "source": [
    "# what works\n",
    "\n",
    "- this build engine notebook uses its own uv env (see toml) including onnx, trtexec etc, and ultralytics. you can pass a .pt file (says onnx support but that hasnt been validated yet as i'm unsure what export commands were used to generate the existing onnx files that lack a valid .pt).\n",
    "\n",
    "- it will export to onnx and then build the TRT engine at full or half precision (not yet setup for int-8 calibration)\n",
    "\n",
    "- i built a custom parser that, conviently, infers the number of keypoints based on the yolo pose head output dims. it works with both stock 11 pose models that output 17 kpt coco shape as well as our custom mousehouse model with 6 kpts\n",
    "\n",
    "- the engine and onnx files built in this ipynb are currently saved to /home/jetson/Desktop/squeakview/new_models/ with the suffix .engine. (**Update to artifacts/weights for pt and artifacts/onnx for onnx ***)\n",
    "\n",
    "- the config created by the ipynb is saved to /home/jetson/Desktop/squeakview/DeepStream-Yolo/configs/ (*** CORRECT place ***)\n",
    "\n",
    "- The custom parser is at DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/yolo_pose_parser.cpp and is now built into DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so. this new parser with do confidence filtering, letterbox UNPadding, and print to the terminal the output tensor dimensions that. the parser is specific to our cuda 12.6 as well so if used on a new cuda version we will need to change that flag and rebuilt the parser using the nvs make file (easy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b9ab2",
   "metadata": {},
   "source": [
    "# to do\n",
    "\n",
    "\n",
    "\n",
    "- check efficiency of this full pipeline (make sure no uncessary copies, mem pressure etc)\n",
    "\n",
    "- also make the keypints look better\n",
    "\n",
    "- uncap workspace size (wont work at fp32 if capped)\n",
    "\n",
    "- avoid commented lines in the config make as they can be read \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d207b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task       : pose\n",
      "Input model: /home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/weights/mousehouse_pose.pt\n",
      "Engine out : /home/jetson/Desktop/squeakview/DeepStream-Yolo/engines/mousehouse_pose_fp32.engine\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"/home/jetson/Desktop/squeakview/DeepStream-Yolo\")\n",
    "PT_DIR = BASE / \"artifacts\" / \"weights\"\n",
    "ONNX_DIR = BASE / \"artifacts\" / \"onnx\"\n",
    "ENGINE_DIR = BASE / \"engines\"\n",
    "\n",
    "# ---- Configure your model here ----------------------------------------\n",
    "# Task: \"pose\" or \"detect\" (controls parser in generated config)\n",
    "TASK = \"pose\"\n",
    "\n",
    "# Path to a .pt or .onnx file (defaults to weights dir)\n",
    "INPUT_MODEL = PT_DIR / \"mousehouse_pose.pt\"\n",
    "\n",
    "# Where to write the engine (defaults to ENGINE_DIR/<stem>_<precision>.engine)\n",
    "ENGINE_OUTPUT = None  # or ENGINE_DIR / \"custom.engine\"\n",
    "\n",
    "# Network input shape (min/opt/max). Keep consistent with DeepStream infer-dims.\n",
    "IMG_SIZE = (1, 3, 640, 640)\n",
    "\n",
    "# Precision: \"fp32\", \"fp16\", or \"int8\" (int8 requires calibration cache not covered here)\n",
    "PRECISION = \"fp32\"\n",
    "\n",
    "# Optional custom plugin used by DeepStream YOLO parser (not required for build, but harmless).\n",
    "CUSTOM_PLUGIN = BASE / \"nvdsinfer_custom_impl_Yolo\" / \"libnvdsinfer_custom_impl_Yolo.so\"\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "for d in (PT_DIR, ONNX_DIR, ENGINE_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PREC_SUFFIX = PRECISION.lower()\n",
    "\n",
    "INPUT_MODEL = INPUT_MODEL.expanduser().resolve()\n",
    "if ENGINE_OUTPUT is None:\n",
    "    ENGINE_OUTPUT = (ENGINE_DIR / f\"{INPUT_MODEL.stem}_{PREC_SUFFIX}\").with_suffix(\".engine\")\n",
    "ENGINE_OUTPUT = ENGINE_OUTPUT.expanduser().resolve()\n",
    "ENGINE_OUTPUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Task       :\", TASK)\n",
    "print(\"Input model:\", INPUT_MODEL)\n",
    "print(\"Engine out :\", ENGINE_OUTPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d877c6",
   "metadata": {},
   "source": [
    "# verify trtexec is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c539abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using trtexec: /usr/src/tensorrt/bin/trtexec\n"
     ]
    }
   ],
   "source": [
    "def find_trtexec() -> Path:\n",
    "    candidates = [\n",
    "        Path(os.environ.get(\"TRTEXEC\") or \"\"),\n",
    "        Path(\"/usr/src/tensorrt/bin/trtexec\"),\n",
    "        Path(shutil.which(\"trtexec\") or \"\"),\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if path and path.is_file() and os.access(path, os.X_OK):\n",
    "            return path\n",
    "    raise FileNotFoundError(\"trtexec not found. Set TRTEXEC env var or install TensorRT tools.\")\n",
    "\n",
    "TRTEXEC = find_trtexec()\n",
    "print(\"Using trtexec:\", TRTEXEC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6895b27",
   "metadata": {},
   "source": [
    "# Export .pt --> Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2666b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting .pt â†’ ONNX: /home/jetson/Desktop/squeakview/build-engine/.venv/bin/yolo export model=/home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/weights/mousehouse_pose.pt format=onnx imgsz=640 simplify=True nms=False\n",
      "Ultralytics 8.3.235 ðŸš€ Python-3.10.12 torch-2.9.1+cpu CPU (ARMv8 Processor rev 1 (v8l))\n",
      "YOLO11n-pose summary (fused): 109 layers, 2,664,805 parameters, 0 gradients, 6.6 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/weights/mousehouse_pose.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 23, 8400) (5.5 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0,<=1.19.1'] not found, attempting AutoUpdate...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnx<=1.19.1,>=1.12.0 in /home/jetson/.local/lib/python3.10/site-packages (1.19.1)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/jetson/.local/lib/python3.10/site-packages (from onnx<=1.19.1,>=1.12.0) (1.24.4)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /home/jetson/.local/lib/python3.10/site-packages (from onnx<=1.19.1,>=1.12.0) (6.33.1)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /home/jetson/.local/lib/python3.10/site-packages (from onnx<=1.19.1,>=1.12.0) (4.15.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /home/jetson/.local/lib/python3.10/site-packages (from onnx<=1.19.1,>=1.12.0) (0.5.4)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 1.6s\n",
      "WARNING âš ï¸ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.20.0 opset 22...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.78...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-12-04 10:22:16.309639024 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card1/device/vendor\"\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 5.4s, saved as '/home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/weights/mousehouse_pose.onnx' (10.5 MB)\n",
      "\n",
      "Export complete (7.2s)\n",
      "Results saved to \u001b[1m/home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/weights\u001b[0m\n",
      "Predict:         yolo predict task=pose model=/home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/weights/mousehouse_pose.onnx imgsz=640  \n",
      "Validate:        yolo val task=pose model=/home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/weights/mousehouse_pose.onnx imgsz=640 data=C:/Users/david/OneDrive/Desktop/CALEB_POSE_PAPER_N/squeakpose_test_paper/Squeakpose Tool Final/datasets/pose\\dataset.yaml  \n",
      "Visualize:       https://netron.app\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/export\n",
      "ONNX written to: /home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/onnx/mousehouse_pose_fp32.onnx\n"
     ]
    }
   ],
   "source": [
    "# If INPUT_MODEL is .pt, convert to ONNX using Ultralytics CLI\n",
    "# (skipped if INPUT_MODEL is already ONNX)\n",
    "def convert_pt_to_onnx(pt_path: Path, imgsz: tuple[int, int]) -> Path:\n",
    "    # Ultralytics writes the ONNX next to the .pt by default; we then move/rename to ONNX_DIR with precision suffix\n",
    "    exported_onnx = pt_path.with_suffix(\".onnx\")\n",
    "    onnx_out = (ONNX_DIR / f\"{pt_path.stem}_{PREC_SUFFIX}\").with_suffix(\".onnx\")\n",
    "    yolo_cmd = shutil.which(\"yolo\")\n",
    "    if not yolo_cmd:\n",
    "        raise RuntimeError(\n",
    "            \"Ultralytics 'yolo' CLI not found. Install with 'pip install ultralytics' in this env, \"\n",
    "            \"or set INPUT_MODEL to an existing ONNX path to skip conversion.\"\n",
    "        )\n",
    "    cmd = [\n",
    "        yolo_cmd,\n",
    "        \"export\",\n",
    "        f\"model={pt_path}\",\n",
    "        \"format=onnx\",\n",
    "        f\"imgsz={imgsz[0] if isinstance(imgsz, (tuple, list)) else imgsz}\",\n",
    "        \"simplify=True\",\n",
    "        \"nms=False\"\n",
    "    ]\n",
    "    print(\"Converting .pt â†’ ONNX:\", \" \".join(cmd))\n",
    "    res = subprocess.run(cmd, text=True)\n",
    "    if res.returncode != 0:\n",
    "        raise RuntimeError(\"Ultralytics conversion failed; ensure 'ultralytics' is installed and yolo CLI is in PATH\")\n",
    "    if not exported_onnx.exists():\n",
    "        raise FileNotFoundError(f\"Expected ONNX not found where Ultralytics writes it: {exported_onnx}\")\n",
    "    onnx_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if onnx_out.resolve() != exported_onnx.resolve():\n",
    "        shutil.move(str(exported_onnx), str(onnx_out))\n",
    "    return onnx_out\n",
    "\n",
    "if INPUT_MODEL.suffix.lower() == \".pt\":\n",
    "    INPUT_MODEL = convert_pt_to_onnx(INPUT_MODEL, imgsz=IMG_SIZE[-2:])\n",
    "    print(\"ONNX written to:\", INPUT_MODEL)\n",
    "else:\n",
    "    assert INPUT_MODEL.exists(), f\"Model not found: {INPUT_MODEL}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d099d",
   "metadata": {},
   "source": [
    "# Verify onnx model tensor name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b81fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX input name: images\n"
     ]
    }
   ],
   "source": [
    "# Determine ONNX input tensor name (DeepStream auto-detects this internally)\n",
    "INPUT_NAME_OVERRIDE = None  # set to e.g. \"images\" to force a name if detection fails\n",
    "\n",
    "def get_onnx_input_name(path: Path) -> str:\n",
    "    try:\n",
    "        import onnx  # type: ignore\n",
    "    except ImportError as exc:\n",
    "        raise RuntimeError(\"onnx is required to auto-detect input name; set INPUT_NAME_OVERRIDE to skip detection\") from exc\n",
    "    model = onnx.load(str(path))\n",
    "    if model.graph.input:\n",
    "        return model.graph.input[0].name\n",
    "    initializers = {init.name for init in model.graph.initializer}\n",
    "    for node in model.graph.node:\n",
    "        for name in node.input:\n",
    "            if name not in initializers:\n",
    "                return name\n",
    "    raise RuntimeError(\"No inputs found in ONNX (graph.input empty and no candidate from nodes)\")\n",
    "\n",
    "INPUT_NAME = INPUT_NAME_OVERRIDE or get_onnx_input_name(INPUT_MODEL)\n",
    "print(\"ONNX input name:\", INPUT_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52cd40",
   "metadata": {},
   "source": [
    "# Build TRT Engine (this may take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f81fffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static input: ['1', '3', '640', '640']\n",
      "Running: /usr/src/tensorrt/bin/trtexec --onnx=/home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/onnx/mousehouse_pose_fp32.onnx --saveEngine=/home/jetson/Desktop/squeakview/DeepStream-Yolo/engines/mousehouse_pose_fp32.engine --staticPlugins=/home/jetson/Desktop/squeakview/DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so\n",
      "&&&& RUNNING TensorRT.trtexec [TensorRT v100300] # /usr/src/tensorrt/bin/trtexec --onnx=/home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/onnx/mousehouse_pose_fp32.onnx --saveEngine=/home/jetson/Desktop/squeakview/DeepStream-Yolo/engines/mousehouse_pose_fp32.engine --staticPlugins=/home/jetson/Desktop/squeakview/DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so\n",
      "[12/04/2025-10:22:25] [I] === Model Options ===\n",
      "[12/04/2025-10:22:25] [I] Format: ONNX\n",
      "[12/04/2025-10:22:25] [I] Model: /home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/onnx/mousehouse_pose_fp32.onnx\n",
      "[12/04/2025-10:22:25] [I] Output:\n",
      "[12/04/2025-10:22:25] [I] === Build Options ===\n",
      "[12/04/2025-10:22:25] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default, tacticSharedMem: default\n",
      "[12/04/2025-10:22:25] [I] avgTiming: 8\n",
      "[12/04/2025-10:22:25] [I] Precision: FP32\n",
      "[12/04/2025-10:22:25] [I] LayerPrecisions: \n",
      "[12/04/2025-10:22:25] [I] Layer Device Types: \n",
      "[12/04/2025-10:22:25] [I] Calibration: \n",
      "[12/04/2025-10:22:25] [I] Refit: Disabled\n",
      "[12/04/2025-10:22:25] [I] Strip weights: Disabled\n",
      "[12/04/2025-10:22:25] [I] Version Compatible: Disabled\n",
      "[12/04/2025-10:22:25] [I] ONNX Plugin InstanceNorm: Disabled\n",
      "[12/04/2025-10:22:25] [I] TensorRT runtime: full\n",
      "[12/04/2025-10:22:25] [I] Lean DLL Path: \n",
      "[12/04/2025-10:22:25] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[12/04/2025-10:22:25] [I] Exclude Lean Runtime: Disabled\n",
      "[12/04/2025-10:22:25] [I] Sparsity: Disabled\n",
      "[12/04/2025-10:22:25] [I] Safe mode: Disabled\n",
      "[12/04/2025-10:22:25] [I] Build DLA standalone loadable: Disabled\n",
      "[12/04/2025-10:22:25] [I] Allow GPU fallback for DLA: Disabled\n",
      "[12/04/2025-10:22:25] [I] DirectIO mode: Disabled\n",
      "[12/04/2025-10:22:25] [I] Restricted mode: Disabled\n",
      "[12/04/2025-10:22:25] [I] Skip inference: Disabled\n",
      "[12/04/2025-10:22:25] [I] Save engine: /home/jetson/Desktop/squeakview/DeepStream-Yolo/engines/mousehouse_pose_fp32.engine\n",
      "[12/04/2025-10:22:25] [I] Load engine: \n",
      "[12/04/2025-10:22:25] [I] Profiling verbosity: 0\n",
      "[12/04/2025-10:22:25] [I] Tactic sources: Using default tactic sources\n",
      "[12/04/2025-10:22:25] [I] timingCacheMode: local\n",
      "[12/04/2025-10:22:25] [I] timingCacheFile: \n",
      "[12/04/2025-10:22:25] [I] Enable Compilation Cache: Enabled\n",
      "[12/04/2025-10:22:25] [I] errorOnTimingCacheMiss: Disabled\n",
      "[12/04/2025-10:22:25] [I] Preview Features: Use default preview flags.\n",
      "[12/04/2025-10:22:25] [I] MaxAuxStreams: -1\n",
      "[12/04/2025-10:22:25] [I] BuilderOptimizationLevel: -1\n",
      "[12/04/2025-10:22:25] [I] Calibration Profile Index: 0\n",
      "[12/04/2025-10:22:25] [I] Weight Streaming: Disabled\n",
      "[12/04/2025-10:22:25] [I] Runtime Platform: Same As Build\n",
      "[12/04/2025-10:22:25] [I] Debug Tensors: \n",
      "[12/04/2025-10:22:25] [I] Input(s)s format: fp32:CHW\n",
      "[12/04/2025-10:22:25] [I] Output(s)s format: fp32:CHW\n",
      "[12/04/2025-10:22:25] [I] Input build shapes: model\n",
      "[12/04/2025-10:22:25] [I] Input calibration shapes: model\n",
      "[12/04/2025-10:22:25] [I] === System Options ===\n",
      "[12/04/2025-10:22:25] [I] Device: 0\n",
      "[12/04/2025-10:22:25] [I] DLACore: \n",
      "[12/04/2025-10:22:25] [I] Plugins: /home/jetson/Desktop/squeakview/DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so\n",
      "[12/04/2025-10:22:25] [I] setPluginsToSerialize:\n",
      "[12/04/2025-10:22:25] [I] dynamicPlugins:\n",
      "[12/04/2025-10:22:25] [I] ignoreParsedPluginLibs: 0\n",
      "[12/04/2025-10:22:25] [I] \n",
      "[12/04/2025-10:22:25] [I] === Inference Options ===\n",
      "[12/04/2025-10:22:25] [I] Batch: Explicit\n",
      "[12/04/2025-10:22:25] [I] Input inference shapes: model\n",
      "[12/04/2025-10:22:25] [I] Iterations: 10\n",
      "[12/04/2025-10:22:25] [I] Duration: 3s (+ 200ms warm up)\n",
      "[12/04/2025-10:22:25] [I] Sleep time: 0ms\n",
      "[12/04/2025-10:22:25] [I] Idle time: 0ms\n",
      "[12/04/2025-10:22:25] [I] Inference Streams: 1\n",
      "[12/04/2025-10:22:25] [I] ExposeDMA: Disabled\n",
      "[12/04/2025-10:22:25] [I] Data transfers: Enabled\n",
      "[12/04/2025-10:22:25] [I] Spin-wait: Disabled\n",
      "[12/04/2025-10:22:25] [I] Multithreading: Disabled\n",
      "[12/04/2025-10:22:25] [I] CUDA Graph: Disabled\n",
      "[12/04/2025-10:22:25] [I] Separate profiling: Disabled\n",
      "[12/04/2025-10:22:25] [I] Time Deserialize: Disabled\n",
      "[12/04/2025-10:22:25] [I] Time Refit: Disabled\n",
      "[12/04/2025-10:22:25] [I] NVTX verbosity: 0\n",
      "[12/04/2025-10:22:25] [I] Persistent Cache Ratio: 0\n",
      "[12/04/2025-10:22:25] [I] Optimization Profile Index: 0\n",
      "[12/04/2025-10:22:25] [I] Weight Streaming Budget: 100.000000%\n",
      "[12/04/2025-10:22:25] [I] Inputs:\n",
      "[12/04/2025-10:22:25] [I] Debug Tensor Save Destinations:\n",
      "[12/04/2025-10:22:25] [I] === Reporting Options ===\n",
      "[12/04/2025-10:22:25] [I] Verbose: Disabled\n",
      "[12/04/2025-10:22:25] [I] Averages: 10 inferences\n",
      "[12/04/2025-10:22:25] [I] Percentiles: 90,95,99\n",
      "[12/04/2025-10:22:25] [I] Dump refittable layers:Disabled\n",
      "[12/04/2025-10:22:25] [I] Dump output: Disabled\n",
      "[12/04/2025-10:22:25] [I] Profile: Disabled\n",
      "[12/04/2025-10:22:25] [I] Export timing to JSON file: \n",
      "[12/04/2025-10:22:25] [I] Export output to JSON file: \n",
      "[12/04/2025-10:22:25] [I] Export profile to JSON file: \n",
      "[12/04/2025-10:22:25] [I] \n",
      "[12/04/2025-10:22:25] [I] === Device Information ===\n",
      "[12/04/2025-10:22:25] [I] Available Devices: \n",
      "[12/04/2025-10:22:25] [I]   Device 0: \"Orin\" UUID: GPU-9f1501e7-94bf-5c7e-b4b6-ee0c3f745616\n",
      "[12/04/2025-10:22:25] [I] Selected Device: Orin\n",
      "[12/04/2025-10:22:25] [I] Selected Device ID: 0\n",
      "[12/04/2025-10:22:25] [I] Selected Device UUID: GPU-9f1501e7-94bf-5c7e-b4b6-ee0c3f745616\n",
      "[12/04/2025-10:22:25] [I] Compute Capability: 8.7\n",
      "[12/04/2025-10:22:25] [I] SMs: 8\n",
      "[12/04/2025-10:22:25] [I] Device Global Memory: 7619 MiB\n",
      "[12/04/2025-10:22:25] [I] Shared Memory per SM: 164 KiB\n",
      "[12/04/2025-10:22:25] [I] Memory Bus Width: 128 bits (ECC disabled)\n",
      "[12/04/2025-10:22:25] [I] Application Compute Clock Rate: 1.02 GHz\n",
      "[12/04/2025-10:22:25] [I] Application Memory Clock Rate: 1.02 GHz\n",
      "[12/04/2025-10:22:25] [I] \n",
      "[12/04/2025-10:22:25] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[12/04/2025-10:22:25] [I] \n",
      "[12/04/2025-10:22:25] [I] TensorRT version: 10.3.0\n",
      "[12/04/2025-10:22:25] [I] Loading standard plugins\n",
      "[12/04/2025-10:22:25] [I] Loading supplied plugin library: /home/jetson/Desktop/squeakview/DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so\n",
      "[12/04/2025-10:22:25] [I] [TRT] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 31, GPU 5300 (MiB)\n",
      "[12/04/2025-10:22:32] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +928, GPU +681, now: CPU 1002, GPU 6026 (MiB)\n",
      "[12/04/2025-10:22:32] [I] Start parsing network model.\n",
      "[12/04/2025-10:22:32] [I] [TRT] ----------------------------------------------------------------\n",
      "[12/04/2025-10:22:32] [I] [TRT] Input filename:   /home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/onnx/mousehouse_pose_fp32.onnx\n",
      "[12/04/2025-10:22:32] [I] [TRT] ONNX IR version:  0.0.10\n",
      "[12/04/2025-10:22:32] [I] [TRT] Opset version:    22\n",
      "[12/04/2025-10:22:32] [I] [TRT] Producer name:    pytorch\n",
      "[12/04/2025-10:22:32] [I] [TRT] Producer version: 2.9.1\n",
      "[12/04/2025-10:22:32] [I] [TRT] Domain:           \n",
      "[12/04/2025-10:22:32] [I] [TRT] Model version:    0\n",
      "[12/04/2025-10:22:32] [I] [TRT] Doc string:       \n",
      "[12/04/2025-10:22:32] [I] [TRT] ----------------------------------------------------------------\n",
      "[12/04/2025-10:22:32] [I] Finished parsing network model. Parse time: 0.0517059\n",
      "[12/04/2025-10:22:33] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/04/2025-10:27:13] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[12/04/2025-10:27:16] [I] [TRT] Total Host Persistent Memory: 576608\n",
      "[12/04/2025-10:27:16] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[12/04/2025-10:27:16] [I] [TRT] Total Scratch Memory: 2764800\n",
      "[12/04/2025-10:27:16] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 247 steps to complete.\n",
      "[12/04/2025-10:27:16] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 39.0367ms to assign 11 blocks to 247 nodes requiring 19344896 bytes.\n",
      "[12/04/2025-10:27:16] [I] [TRT] Total Activation Memory: 19344384\n",
      "[12/04/2025-10:27:16] [I] [TRT] Total Weights Memory: 10898816\n",
      "[12/04/2025-10:27:16] [I] [TRT] Engine generation completed in 283.249 seconds.\n",
      "[12/04/2025-10:27:16] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 132 MiB\n",
      "[12/04/2025-10:27:16] [I] [TRT] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 1647 MiB\n",
      "[12/04/2025-10:27:16] [I] Engine built in 283.414 sec.\n",
      "[12/04/2025-10:27:16] [I] Created engine with size: 12.5291 MiB\n",
      "[12/04/2025-10:27:16] [I] [TRT] Loaded engine size: 12 MiB\n",
      "[12/04/2025-10:27:16] [I] Engine deserialized in 0.100225 sec.\n",
      "[12/04/2025-10:27:16] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +18, now: CPU 0, GPU 28 (MiB)\n",
      "[12/04/2025-10:27:16] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[12/04/2025-10:27:16] [I] Created execution context with device memory size: 18.4482 MiB\n",
      "[12/04/2025-10:27:16] [I] Using random values for input images\n",
      "[12/04/2025-10:27:16] [I] Input binding for images with dimensions 1x3x640x640 is created.\n",
      "[12/04/2025-10:27:16] [I] Output binding for output0 with dimensions 1x23x8400 is created.\n",
      "[12/04/2025-10:27:16] [I] Starting inference\n",
      "[12/04/2025-10:27:20] [I] Warmup completed 17 queries over 200 ms\n",
      "[12/04/2025-10:27:20] [I] Timing trace has 249 queries over 3.03385 s\n",
      "[12/04/2025-10:27:20] [I] \n",
      "[12/04/2025-10:27:20] [I] === Trace details ===\n",
      "[12/04/2025-10:27:20] [I] Trace averages of 10 runs:\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 10.3742 ms - Host latency: 11.0998 ms (enqueue 3.59243 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 13.0829 ms - Host latency: 13.596 ms (enqueue 2.93127 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 11.4292 ms - Host latency: 11.8711 ms (enqueue 2.9691 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.2334 ms - Host latency: 12.6668 ms (enqueue 3.13781 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.6931 ms - Host latency: 13.1106 ms (enqueue 3.92579 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 11.9897 ms - Host latency: 12.3961 ms (enqueue 3.12499 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 11.9 ms - Host latency: 12.3199 ms (enqueue 3.15604 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.2688 ms - Host latency: 12.6693 ms (enqueue 3.02893 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.2346 ms - Host latency: 12.6386 ms (enqueue 2.66786 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.1999 ms - Host latency: 12.6326 ms (enqueue 3.54751 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.3127 ms - Host latency: 12.7398 ms (enqueue 3.94819 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.3344 ms - Host latency: 12.7492 ms (enqueue 3.29688 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 11.9498 ms - Host latency: 12.3681 ms (enqueue 2.7541 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.4367 ms - Host latency: 12.8594 ms (enqueue 2.55295 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.4728 ms - Host latency: 12.9213 ms (enqueue 3.08536 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 11.9897 ms - Host latency: 12.4386 ms (enqueue 3.47786 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.5357 ms - Host latency: 12.9691 ms (enqueue 3.35234 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.0808 ms - Host latency: 12.4953 ms (enqueue 4.04771 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 11.9498 ms - Host latency: 12.3814 ms (enqueue 3.68809 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.2727 ms - Host latency: 12.6763 ms (enqueue 2.48313 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.1817 ms - Host latency: 12.6074 ms (enqueue 2.4332 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.0685 ms - Host latency: 12.4859 ms (enqueue 2.86326 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.5035 ms - Host latency: 12.9376 ms (enqueue 2.96426 ms)\n",
      "[12/04/2025-10:27:20] [I] Average on 10 runs - GPU latency: 12.1994 ms - Host latency: 12.617 ms (enqueue 3.06038 ms)\n",
      "[12/04/2025-10:27:20] [I] \n",
      "[12/04/2025-10:27:20] [I] === Performance summary ===\n",
      "[12/04/2025-10:27:20] [I] Throughput: 82.0741 qps\n",
      "[12/04/2025-10:27:20] [I] Latency: min = 7.99121 ms, max = 21.936 ms, mean = 12.5844 ms, median = 13.0017 ms, percentile(90%) = 14.4875 ms, percentile(95%) = 14.7087 ms, percentile(99%) = 15.1756 ms\n",
      "[12/04/2025-10:27:20] [I] Enqueue Time: min = 1.91528 ms, max = 7.09656 ms, mean = 3.1639 ms, median = 3.09509 ms, percentile(90%) = 3.95996 ms, percentile(95%) = 4.13977 ms, percentile(99%) = 6.08002 ms\n",
      "[12/04/2025-10:27:20] [I] H2D Latency: min = 0.209229 ms, max = 0.745056 ms, mean = 0.398563 ms, median = 0.398682 ms, percentile(90%) = 0.451538 ms, percentile(95%) = 0.525238 ms, percentile(99%) = 0.699219 ms\n",
      "[12/04/2025-10:27:20] [I] GPU Compute Time: min = 7.50064 ms, max = 21.2096 ms, mean = 12.1457 ms, median = 12.4988 ms, percentile(90%) = 14.0757 ms, percentile(95%) = 14.3264 ms, percentile(99%) = 14.5941 ms\n",
      "[12/04/2025-10:27:20] [I] D2H Latency: min = 0.0336914 ms, max = 0.104187 ms, mean = 0.0402046 ms, median = 0.0401306 ms, percentile(90%) = 0.0441284 ms, percentile(95%) = 0.0457764 ms, percentile(99%) = 0.0695801 ms\n",
      "[12/04/2025-10:27:20] [I] Total Host Walltime: 3.03385 s\n",
      "[12/04/2025-10:27:20] [I] Total GPU Compute Time: 3.02427 s\n",
      "[12/04/2025-10:27:20] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[12/04/2025-10:27:20] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v100300] # /usr/src/tensorrt/bin/trtexec --onnx=/home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/onnx/mousehouse_pose_fp32.onnx --saveEngine=/home/jetson/Desktop/squeakview/DeepStream-Yolo/engines/mousehouse_pose_fp32.engine --staticPlugins=/home/jetson/Desktop/squeakview/DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12/04/2025-10:27:20] [W] * GPU compute time is unstable, with coefficient of variance = 15.995%.\n",
      "[12/04/2025-10:27:20] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return code: 0\n"
     ]
    }
   ],
   "source": [
    "# Build the engine with trtexec (TensorRT 10.x flags)\n",
    "import onnx  # ensure onnx is available for shape inspection\n",
    "\n",
    "# Inspect input shape to decide whether to pass explicit shapes\n",
    "model = onnx.load(str(INPUT_MODEL))\n",
    "inp = model.graph.input[0]\n",
    "type_proto = inp.type.tensor_type\n",
    "shape_proto = type_proto.shape\n",
    "\n",
    "dims = []\n",
    "dynamic = False\n",
    "for dim in shape_proto.dim:\n",
    "    if dim.HasField(\"dim_param\") or dim.dim_param:\n",
    "        dynamic = True\n",
    "        dims.append(dim.dim_param or \"-1\")\n",
    "    else:\n",
    "        val = dim.dim_value\n",
    "        if val in (0, None):\n",
    "            dynamic = True\n",
    "            dims.append(\"-1\")\n",
    "        else:\n",
    "            dims.append(str(val))\n",
    "            if val < 0:\n",
    "                dynamic = True\n",
    "\n",
    "shape_str = \"x\".join(dims) if dynamic else \"x\".join(str(v) for v in IMG_SIZE)\n",
    "\n",
    "cmd = [\n",
    "    str(TRTEXEC),\n",
    "    f\"--onnx={INPUT_MODEL}\",\n",
    "    f\"--saveEngine={ENGINE_OUTPUT}\",\n",
    "]\n",
    "\n",
    "# Only add shape profile flags if the model is dynamic\n",
    "if dynamic:\n",
    "    cmd.extend([\n",
    "        f\"--minShapes={INPUT_NAME}:{shape_str}\",\n",
    "        f\"--optShapes={INPUT_NAME}:{shape_str}\",\n",
    "        f\"--maxShapes={INPUT_NAME}:{shape_str}\",\n",
    "    ])\n",
    "\n",
    "prec = PRECISION.lower()\n",
    "if prec == \"fp16\":\n",
    "    cmd.append(\"--fp16\")\n",
    "elif prec == \"int8\":\n",
    "    cmd.append(\"--int8\")\n",
    "elif prec == \"fp32\":\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(\"PRECISION must be one of fp32, fp16, int8\")\n",
    "\n",
    "if CUSTOM_PLUGIN and CUSTOM_PLUGIN.exists():\n",
    "    cmd.append(f\"--staticPlugins={CUSTOM_PLUGIN}\")  # replaces --plugins\n",
    "\n",
    "print(\"Dynamic input:\" if dynamic else \"Static input:\", dims)\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "result = subprocess.run(cmd)\n",
    "print(\"Return code:\", result.returncode)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"trtexec failed; check above logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31cca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config written to /home/jetson/Desktop/squeakview/DeepStream-Yolo/configs/mousehouse_pose_fp32.txt\n",
      "Parser: NvDsInferParseYoloV8Pose\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Auto-generate config without relying on template placeholders\n",
    "CFG_DIR = Path(\"/home/jetson/Desktop/squeakview/DeepStream-Yolo/configs\")\n",
    "CFG_NAME = f\"{ENGINE_OUTPUT.stem}.txt\"\n",
    "CFG_PATH = CFG_DIR / CFG_NAME\n",
    "\n",
    "is_pose = str(TASK).lower() == \"pose\"\n",
    "parse_func = \"NvDsInferParseYoloV8Pose\" if is_pose else \"NvDsInferParseYolo\"\n",
    "# class labels for detect; kp labels for pose\n",
    "labels_path = Path(\"/home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/labels/mouse_class.txt\")\n",
    "kp_labels_path = Path(\"/home/jetson/Desktop/squeakview/DeepStream-Yolo/artifacts/labels/mouse_labels.txt\") if is_pose else None\n",
    "\n",
    "network_mode = 2 if PRECISION.lower() == \"fp16\" else 0\n",
    "infer_h, infer_w = IMG_SIZE[-2], IMG_SIZE[-1]\n",
    "class_count = 1  # adjust per model\n",
    "\n",
    "config_text = f\"\"\"[property]\n",
    "gpu-id=0\n",
    "net-scale-factor=0.0039215697906911373\n",
    "model-color-format=0\n",
    "\n",
    "onnx-file={INPUT_MODEL}\n",
    "model-engine-file={ENGINE_OUTPUT}\n",
    "\n",
    "network-mode={network_mode}                  # 0=FP32, 1=INT8, 2=FP16\n",
    "network-type=0                  # detector\n",
    "infer-dims=3;{infer_w};{infer_h}\n",
    "batch-size=1\n",
    "output-tensor-meta=1\n",
    "\n",
    "num-detected-classes={class_count}\n",
    "labelfile-path={labels_path}\n",
    "\"\"\"\n",
    "\n",
    "if is_pose and kp_labels_path:\n",
    "    config_text += f\"pose-kpt-labels-path={kp_labels_path}\\n\"\n",
    "\n",
    "config_text += f\"\"\"parse-bbox-func-name={parse_func}\n",
    "custom-lib-path=/home/jetson/Desktop/squeakview/DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so\n",
    "engine-create-func-name=NvDsInferYoloCudaEngineGet\n",
    "\"\"\"\n",
    "\n",
    "if is_pose:\n",
    "    config_text += \"pose-draw-threshold=0.5\\n\"\n",
    "\n",
    "config_text += f\"\"\"\n",
    "cluster-mode=2\n",
    "maintain-aspect-ratio=1\n",
    "symmetric-padding=1\n",
    "workspace-size=2048\n",
    "gie-unique-id=1\n",
    "interval=0\n",
    "process-mode=1\n",
    "\n",
    "[class-attrs-all]\n",
    "nms-iou-threshold=0.15\n",
    "pre-cluster-threshold=0.9\n",
    "topk=20\n",
    "\"\"\"\n",
    "\n",
    "CFG_PATH.write_text(config_text)\n",
    "print(f\"Config written to {CFG_PATH}\")\n",
    "print(\"Parser:\", parse_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471e602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
